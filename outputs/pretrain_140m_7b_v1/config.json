{
  "train_dir": "datasets/pretrain_mix_7b_v1/train",
  "val_dir": "datasets/pretrain_mix_7b_v1/val",
  "out_dir": "outputs/pretrain_140m_7b_v1",
  "samples_dir": "samples/pretrain_140m_7b_v1",
  "tokenizer_path": "tokenizer/tokenizer.json",
  "vocab_size": 32000,
  "seq_len": 2048,
  "layers": 12,
  "d_model": 768,
  "n_heads": 12,
  "d_ff": 3072,
  "dropout": 0.0,
  "bos_id": 2,
  "eos_id": 3,
  "no_mask_bos_in_loss": false,
  "no_mask_last_label_in_loss": false,
  "eos_weight": 1.0,
  "eos_weight_warmup_steps": 0,
  "precision": "bf16",
  "micro_bsz": 10,
  "grad_accum": 4,
  "lr": 3e-05,
  "weight_decay": 0.1,
  "warmup_steps": 6000,
  "max_steps": 500000,
  "grad_clip": 1.0,
  "num_workers": 2,
  "seed": 1234,
  "log_every": 20,
  "eval_every": 100,
  "save_every": 2000,
  "debug_every": 2000,
  "add_bos_to_prompts": false,
  "sample_temperature": 0.6,
  "sample_top_p": 0.85,
  "sample_top_k": 0,
  "sample_max_new_tokens": 256,
  "sample_min_new_tokens": 32,
  "stop_string": [
    "\n\nOnce upon a time",
    "\nOnce upon a time"
  ],
  "strip_stop": true,
  "resume_path": "outputs/pretrain_140m_7b_v1/latest.pt",
  "resume_full": true,
  "resume_step": -1,
  "compile": true,
  "model_cfg": {
    "vocab_size": 32000,
    "n_layers": 12,
    "d_model": 768,
    "n_heads": 12,
    "d_ff": 3072,
    "max_seq_len": 2048,
    "dropout": 0.0,
    "tie_embeddings": true,
    "rope_theta": 10000.0,
    "rope_pct": 1.0
  }
}