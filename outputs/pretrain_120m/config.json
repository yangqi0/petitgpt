{
  "train_dir": "datasets/pretrain_mix/train",
  "val_dir": "datasets/pretrain_mix/val",
  "out_dir": "outputs/pretrain_120m",
  "samples_dir": "samples/pretrain_120m",
  "tokenizer_path": "tokenizer/tokenizer.json",
  "vocab_size": 32000,
  "seq_len": 1024,
  "layers": 12,
  "d_model": 768,
  "n_heads": 12,
  "d_ff": 3072,
  "dropout": 0.0,
  "bos_id": 2,
  "eos_id": 3,
  "no_mask_bos_in_loss": false,
  "no_mask_last_label_in_loss": false,
  "eos_weight": 1.0,
  "eos_weight_warmup_steps": 0,
  "precision": "bf16",
  "micro_bsz": 4,
  "grad_accum": 8,
  "lr": 5e-05,
  "weight_decay": 0.1,
  "warmup_steps": 1000,
  "max_steps": 80000,
  "grad_clip": 1.0,
  "num_workers": 2,
  "seed": 1234,
  "log_every": 20,
  "eval_every": 500,
  "save_every": 2000,
  "debug_every": 500,
  "add_bos_to_prompts": false,
  "sample_temperature": 0.7,
  "sample_top_p": 0.9,
  "sample_top_k": 0,
  "sample_max_new_tokens": 256,
  "sample_min_new_tokens": 32,
  "resume_path": "",
  "resume_full": false,
  "compile": false,
  "model_cfg": {
    "vocab_size": 32000,
    "n_layers": 12,
    "d_model": 768,
    "n_heads": 12,
    "d_ff": 3072,
    "max_seq_len": 1024,
    "dropout": 0.0,
    "tie_embeddings": true
  }
}