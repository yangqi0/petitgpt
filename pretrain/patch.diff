diff --git a/pretrain/sample.py b/pretrain/sample.py
index 1111111..2222222 100644
--- a/pretrain/sample.py
+++ b/pretrain/sample.py
@@ -1,6 +1,7 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 Sampling / text generation for petitgpt.
@@
 from __future__ import annotations

 import argparse
 import json
+import re
 import random
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple

@@
 class StopConfig:
@@
-    stop_strings: List[str]
+    stop_strings: List[str]
+    stop_regexes: List["re.Pattern"]
     stop_on_newline: bool = False
     include_stop_in_output: bool = False


-def _postprocess_stop(text: str, stop_cfg: StopConfig) -> str:
+def _postprocess_stop(text: str, stop_cfg: StopConfig) -> str:
@@
     return text


 @torch.no_grad()
 def generate(
@@
     extra_ban_token_ids: Optional[List[int]] = None,
     # stopping:
     stop_strings: Optional[List[str]] = None,
     stop_on_newline: bool = False,
     include_stop_in_output: bool = False,
+    stop_regex: Optional[List[str]] = None,
 ) -> Dict[str, Any]:
@@
-    stop_cfg = StopConfig(
-        stop_strings=list(stop_strings or []),
-        stop_on_newline=bool(stop_on_newline),
-        include_stop_in_output=bool(include_stop_in_output),
-    )
+    # compile regex once
+    compiled: List[re.Pattern] = []
+    for pat in (stop_regex or []):
+        if not pat:
+            continue
+        compiled.append(re.compile(pat))
+
+    stop_cfg = StopConfig(
+        stop_strings=list(stop_strings or []),
+        stop_regexes=compiled,
+        stop_on_newline=bool(stop_on_newline),
+        include_stop_in_output=bool(include_stop_in_output),
+    )

     dbg: Dict[str, Any] = {}
     if debug:
@@
         dbg["stop_on_newline"] = bool(stop_cfg.stop_on_newline)
         dbg["stop_strings"] = stop_cfg.stop_strings
+        dbg["stop_regex"] = (stop_regex or [])
         dbg["include_stop_in_output"] = bool(stop_cfg.include_stop_in_output)
@@
-    gen_text_buf: str = ""
-    stopped_by: Optional[str] = None
+    gen_text_buf: str = ""   # generated text only (no prompt)
+    stopped_by: Optional[str] = None
+    stop_cut: Optional[int] = None  # truncate generated text to this length (in chars)

     for step in range(max_new_tokens):
@@
         generated.append(nxt)
         ids = torch.cat([ids, torch.tensor([[nxt]], device=device, dtype=torch.long)], dim=1)

         # ---- string-level stop detection ----
-        if stop_cfg.stop_on_newline or stop_cfg.stop_strings:
+        if stop_cfg.stop_on_newline or stop_cfg.stop_strings or stop_cfg.stop_regexes:
             try:
                 piece = tokenizer.decode([nxt])
             except Exception:
                 piece = ""
             if piece:
                 gen_text_buf += piece

             # stop on newline
             if stopped_by is None and stop_cfg.stop_on_newline and "\n" in gen_text_buf:
                 stopped_by = "\\n"
+                # truncate at first newline unless include_stop_in_output
+                if not stop_cfg.include_stop_in_output:
+                    stop_cut = gen_text_buf.find("\n")

             # stop on strings
             if stopped_by is None and stop_cfg.stop_strings:
                 for s in stop_cfg.stop_strings:
                     if not s:
                         continue
                     j = gen_text_buf.find(s)
                     if j >= 0:
                         stopped_by = s
+                        if not stop_cfg.include_stop_in_output:
+                            stop_cut = j
                         break

+            # stop on regex (truncate to match end by default)
+            if stopped_by is None and stop_cfg.stop_regexes:
+                for rx in stop_cfg.stop_regexes:
+                    m = rx.search(gen_text_buf)
+                    if m is not None:
+                        stopped_by = f"re:{rx.pattern}"
+                        if not stop_cfg.include_stop_in_output:
+                            stop_cut = m.end()
+                        break
+
             if stopped_by is not None and (step + 1) >= min_new_tokens:
                 break

         if eos_id is not None and nxt == eos_id and (step + 1) >= min_new_tokens:
             break

     full_ids = prompt_ids + generated
-    out_text = tokenizer.decode(full_ids)
+    # build final output with optional truncation
+    prompt_text = tokenizer.decode(prompt_ids)
+    gen_text = tokenizer.decode(generated)
+    if stop_cut is not None:
+        gen_text = gen_text[:stop_cut]
+    out_text = prompt_text + gen_text

     return {
         "prompt_tokens": len(prompt_ids),
         "new_tokens": generated,
         "output_text": out_text,
         "debug": dbg if debug else None,
     }
@@
 def main() -> None:
@@
     ap.add_argument("--stop_on_newline", action="store_true", help="Stop when a newline is generated (after min_new_tokens).")
     ap.add_argument("--stop_string", action="append", default=None, help="Stop when this string appears (can repeat).")
+    ap.add_argument("--stop_regex", action="append", default=None, help="Stop when this regex matches generated text (can repeat).")
     ap.add_argument("--include_stop_in_output", action="store_true", help="Keep stop marker; default truncates it.")
@@
             res = generate(
@@
                 stop_strings=args.stop_string,
                 stop_on_newline=args.stop_on_newline,
                 include_stop_in_output=args.include_stop_in_output,
+                stop_regex=args.stop_regex,
             )
@@
     generate_default_samples(
@@
         stop_strings=args.stop_string,
         stop_on_newline=args.stop_on_newline,
         include_stop_in_output=args.include_stop_in_output,
+        stop_regex=args.stop_regex,
         seed_base=args.seed,
         step_tag=ckpt.get("global_step", None),
     )
