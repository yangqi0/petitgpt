Samples generated with tokenizer=tokenizer/tokenizer.json
precision=bf16, temperature=0.7, top_p=0.9, top_k=0, max_new_tokens=256, min_new_tokens=32, greedy=False
================================================================================
[Prompt 1] (prompt_tokens=8)
Once upon a time, 

[Debug]
bos_id=[BOS] eos_id=[EOS]
first_step_eos_prob=0.0003405921452213079
first_step_topk_ids=[72, 266, 15, 1691, 1430, 1731, 2549, 412, 29, 601]
first_step_topk_probs=[0.6733170747756958, 0.024745525792241096, 0.010132900439202785, 0.008862769231200218, 0.008862769231200218, 0.008105747401714325, 0.0070897117257118225, 0.006484140641987324, 0.005671368911862373, 0.004960475955158472]

[New tokens 1] count=181 first30=[72, 15, 261, 3895, 4066, 5728, 2895, 2220, 289, 266, 3907, 349, 566, 1687, 17, 722, 2624, 261, 1576, 4220, 286, 2711, 289, 6335, 354, 17, 2478, 1687, 836, 15]
[New text 1] repr='e, a boy named Timmy went to the park with his mom. He saw a big tree and wanted to climb it. His mom said, "Be careful, Timmy! The tree is very high." Timmy climbed the tree and got very high. He felt like a bird. Suddenly, a man came up to him and said, "Hello, little boy! What are you doing here?" Timmy said, "I\'m trying to climb the tree, but I can\'t reach it." The man said, "Don\'t worry, I\'ll help you." He climbed the tree and got the leash. Timmy was so happy and said, "Thank you, mister!"'

[Full output 1]
 Once upon a time, e, a boy named Timmy went to the park with his mom. He saw a big tree and wanted to climb it. His mom said, "Be careful, Timmy! The tree is very high." Timmy climbed the tree and got very high. He felt like a bird. Suddenly, a man came up to him and said, "Hello, little boy! What are you doing here?" Timmy said, "I'm trying to climb the tree, but I can't reach it." The man said, "Don't worry, I'll help you." He climbed the tree and got the leash. Timmy was so happy and said, "Thank you, mister!" The man smiled and said, "You're welcome, little boy. Now, let's go home and enjoy the park." Timmy was happy and said, "Thank you, mister!"

================================================================================
[Prompt 2] (prompt_tokens=11)
In a distant future, humans and robots 

[Debug]
bos_id=[BOS] eos_id=[EOS]
first_step_eos_prob=0.0004876480670645833
first_step_topk_ids=[72, 15, 266, 828, 286, 1584, 86, 988, 412, 3063]
first_step_topk_probs=[0.7711707949638367, 0.028341814875602722, 0.017344361171126366, 0.012135372497141361, 0.005196108017116785, 0.0038015537429600954, 0.003635573899373412, 0.003179864725098014, 0.0027812763582915068, 0.00243265088647604]

[New tokens 2] count=256 first30=[72, 17, 302, 17, 74, 17, 412, 1766, 17, 302, 17, 74, 17, 412, 1766, 17, 302, 17, 74, 17, 412, 1766, 17, 302, 17, 74, 17, 412, 1766, 17]
[New text 2] repr='e. e.g. your name. e.g. your name. e.g. your name. e.g. your name. e.g. your name. e.g. e. e.g. your name. e.g. your name. e.g. your name. e.e. your name. e.e. e.e. your name. e.e. your name. e.e. e.e. e.e. e.e. your name. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e.'

[Full output 2]
 In a distant future, humans and robots e. e.g. your name. e.g. your name. e.g. your name. e.g. your name. e.g. your name. e.g. e. e.g. your name. e.g. your name. e.g. your name. e.e. your name. e.e. e.e. your name. e.e. your name. e.e. e.e. e.e. e.e. your name. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e. e.e.

================================================================================
[Prompt 3] (prompt_tokens=11)
The following is a news report:



[Debug]
bos_id=[BOS] eos_id=[EOS]
first_step_eos_prob=0.00010944751556962729
first_step_topk_ids=[1584, 698, 72, 6892, 988, 1573, 5742, 29, 1691, 15]
first_step_topk_probs=[0.06130214408040047, 0.04689719155430794, 0.041018739342689514, 0.03281267359852791, 0.02400626428425312, 0.02195574901998043, 0.02099713496863842, 0.015361832454800606, 0.01343626994639635, 0.012288598343729973]

[New tokens 3] count=256 first30=[436, 261, 1671, 2694, 292, 266, 2063, 292, 266, 2063, 292, 266, 2063, 17, 351, 1781, 332, 261, 1140, 292, 266, 2063, 338, 504, 261, 6074, 2003, 290, 266, 2063]
[New text 3] repr=' have a clear analysis of the language of the language of the language. The following is a list of the language that has a distinct value in the language of the language. It is also used to teach the language of the language. The language of the language is used to teach the language of the language. In this regard, the language of the language is used to teach the language of the language of the language. The language of the language is used to teach the language of the language of the language'

[Full output 3]
 The following is a news report:

 have a clear analysis of the language of the language of the language. The following is a list of the language that has a distinct value in the language of the language. It is also used to teach the language of the language. The language of the language is used to teach the language of the language. In this regard, the language of the language is used to teach the language of the language of the language. The language of the language is used to teach the language of the language of the language. The language of the language is used to teach the language of the language of the language. The language of the language is used to teach the language of the language of the language. The language of the language is used to teach the language of the language of the language. The language of the language of the language is used to teach the language of the language of the language. The language of the language of the language is used to teach the language of the language of the language. The language of the language of the language is used to teach the language of the language of the language. The language of the language of the language is used to teach the language of the language of the language. The language of the language of the language is used to teach the language of the language

================================================================================
[Prompt 4] (prompt_tokens=14)
Neural networks are a class of machine learning models that 

[Debug]
bos_id=[BOS] eos_id=[EOS]
first_step_eos_prob=0.0014819848584011197
first_step_topk_ids=[72, 828, 266, 15, 1102, 559, 5794, 6892, 86, 34]
first_step_topk_probs=[0.2629472613334656, 0.03225713595747948, 0.03225713595747948, 0.02467726171016693, 0.022569429129362106, 0.01974041573703289, 0.01805426925420761, 0.012080546468496323, 0.008838316425681114, 0.007904951460659504]

[New tokens 4] count=76 first30=[1102, 292, 412, 1102, 15, 351, 1218, 2720, 6519, 332, 261, 1298, 292, 1514, 286, 261, 1216, 292, 6709, 445, 1055, 266, 897, 17, 901, 371, 261, 1216, 292, 5539]
[New text 4] repr=' home of your home, The New York Times is a class of research and a group of scientists from around the world. They are a group of researchers who are highly talented at learning and learning. The world is a leading source of technology and technology. The world is a leading source of technology and technology. The world is a leading source of technology and technology for the world.'

[Full output 4]
 Neural networks are a class of machine learning models that  home of your home, The New York Times is a class of research and a group of scientists from around the world. They are a group of researchers who are highly talented at learning and learning. The world is a leading source of technology and technology. The world is a leading source of technology and technology. The world is a leading source of technology and technology for the world.

================================================================================
[Prompt 5] (prompt_tokens=21)
Here is a short Python snippet:

def fib(n):
    

[Debug]
bos_id=[BOS] eos_id=[EOS]
first_step_eos_prob=0.005883571691811085
first_step_topk_ids=[29, 15, 266, 434, 17, 371, 224, 72, 3455, 828]
first_step_topk_probs=[0.14972056448459625, 0.10475540906190872, 0.08762413263320923, 0.08379841595888138, 0.07664070278406143, 0.04289567843079567, 0.02870252914726734, 0.012289818376302719, 0.010279991663992405, 0.009831160306930542]

[New tokens 5] count=256 first30=[1782, 29, 5657, 285, 29, 1633, 17, 320, 17, 1506, 69, 17, 426, 17, 1886, 17, 434, 58, 17, 320, 17, 434, 48, 17, 320, 17, 434, 58, 17, 320]
[New text 5] repr=' study: Formal: Z. S. Isb. L. No. (W. S. (M. S. (W. S. (W. S. (H. S. (M. S. S. (A. S. S. L. S. (H. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S.'

[Full output 5]
 Here is a short Python snippet:

def fib(n):
     study: Formal: Z. S. Isb. L. No. (W. S. (M. S. (W. S. (W. S. (H. S. (M. S. S. (A. S. S. L. S. (H. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S.

